{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qbP7sOTmoNwy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Set path to your audio dataset\n",
        "data_path = '/content/drive/MyDrive/speech_data/speech_data/'\n",
        "\n",
        "# Parameters for loading and processing audio\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 2  # seconds\n",
        "N_MFCC = 13\n",
        "\n",
        "# Helper function to extract features from audio files\n",
        "def extract_features(file_path, sample_rate=SAMPLE_RATE, duration=DURATION, n_mfcc=N_MFCC):\n",
        "    audio, _ = librosa.load(file_path, sr=sample_rate, duration=duration)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    return np.mean(mfccs.T, axis=0)\n",
        "\n",
        "# Prepare dataset and labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Loop over all classes (folders)\n",
        "for label in os.listdir(data_path):\n",
        "    class_dir = os.path.join(data_path, label)\n",
        "    if os.path.isdir(class_dir):\n",
        "        # Loop over all audio files in class folder\n",
        "        for file in os.listdir(class_dir):\n",
        "            if file.endswith('.wav'):\n",
        "                file_path = os.path.join(class_dir, file)\n",
        "                features = extract_features(file_path)\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Encode labels to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected number of output neurons to match the number of classes\n",
        "num_classes = len(np.unique(y_encoded))  # Should be 35 in your case\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(N_MFCC,)),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax')  # Set output to 35 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y_categorical, epochs=30, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JShkPjctr3sK",
        "outputId": "d97a0a7b-3db2-4cfb-b73a-d471bcadf809"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.0393 - loss: 74.6269 - val_accuracy: 0.0000e+00 - val_loss: 5.1390\n",
            "Epoch 2/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0302 - loss: 13.9594 - val_accuracy: 0.0000e+00 - val_loss: 3.5808\n",
            "Epoch 3/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0345 - loss: 4.2178 - val_accuracy: 0.0000e+00 - val_loss: 3.6073\n",
            "Epoch 4/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0332 - loss: 3.7113 - val_accuracy: 0.0000e+00 - val_loss: 3.6338\n",
            "Epoch 5/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0390 - loss: 3.5811 - val_accuracy: 0.0000e+00 - val_loss: 3.6597\n",
            "Epoch 6/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0400 - loss: 3.7454 - val_accuracy: 0.0000e+00 - val_loss: 3.6843\n",
            "Epoch 7/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0361 - loss: 3.6359 - val_accuracy: 0.0000e+00 - val_loss: 3.7086\n",
            "Epoch 8/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0364 - loss: 3.5664 - val_accuracy: 0.0000e+00 - val_loss: 3.7328\n",
            "Epoch 9/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0414 - loss: 3.5602 - val_accuracy: 0.0000e+00 - val_loss: 3.7562\n",
            "Epoch 10/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0349 - loss: 3.5743 - val_accuracy: 0.0000e+00 - val_loss: 3.7794\n",
            "Epoch 11/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0433 - loss: 3.5172 - val_accuracy: 0.0000e+00 - val_loss: 3.8024\n",
            "Epoch 12/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0382 - loss: 3.5318 - val_accuracy: 0.0000e+00 - val_loss: 3.8254\n",
            "Epoch 13/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0269 - loss: 3.5423 - val_accuracy: 0.0000e+00 - val_loss: 3.8479\n",
            "Epoch 14/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0474 - loss: 3.5010 - val_accuracy: 0.0000e+00 - val_loss: 3.8700\n",
            "Epoch 15/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0487 - loss: 3.5075 - val_accuracy: 0.0000e+00 - val_loss: 3.8922\n",
            "Epoch 16/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0433 - loss: 3.4785 - val_accuracy: 0.0000e+00 - val_loss: 3.9142\n",
            "Epoch 17/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0341 - loss: 3.4716 - val_accuracy: 0.0000e+00 - val_loss: 3.9358\n",
            "Epoch 18/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0481 - loss: 3.4732 - val_accuracy: 0.0000e+00 - val_loss: 3.9568\n",
            "Epoch 19/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0571 - loss: 3.4904 - val_accuracy: 0.0000e+00 - val_loss: 3.9780\n",
            "Epoch 20/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0502 - loss: 3.4708 - val_accuracy: 0.0000e+00 - val_loss: 3.9992\n",
            "Epoch 21/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0478 - loss: 3.4582 - val_accuracy: 0.0000e+00 - val_loss: 4.0190\n",
            "Epoch 22/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0552 - loss: 3.4490 - val_accuracy: 0.0000e+00 - val_loss: 4.0397\n",
            "Epoch 23/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0474 - loss: 3.4487 - val_accuracy: 0.0000e+00 - val_loss: 4.0601\n",
            "Epoch 24/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0421 - loss: 3.4464 - val_accuracy: 0.0000e+00 - val_loss: 4.0804\n",
            "Epoch 25/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0445 - loss: 3.4563 - val_accuracy: 0.0000e+00 - val_loss: 4.1005\n",
            "Epoch 26/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0459 - loss: 3.4447 - val_accuracy: 0.0000e+00 - val_loss: 4.1202\n",
            "Epoch 27/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0420 - loss: 3.4469 - val_accuracy: 0.0000e+00 - val_loss: 4.1401\n",
            "Epoch 28/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0402 - loss: 3.4497 - val_accuracy: 0.0000e+00 - val_loss: 4.1596\n",
            "Epoch 29/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0467 - loss: 3.4304 - val_accuracy: 0.0000e+00 - val_loss: 4.1791\n",
            "Epoch 30/30\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0434 - loss: 3.4319 - val_accuracy: 0.0000e+00 - val_loss: 4.1982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "evaluation = model.evaluate(X, y_categorical)\n",
        "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
        "\n",
        "# Save the model for future use\n",
        "model.save('fine_tuned_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufKtmRIir4rZ",
        "outputId": "a91446f8-c0e1-49dc-ddba-0bf1306c8187"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0873 - loss: 3.4353     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.5790865421295166, Test Accuracy: 0.03465346619486809\n"
          ]
        }
      ]
    }
  ]
}